# Multimodal Retrieval using CLIP + FAISS  
A simple Python pipeline that reads **texts and images**, embeds them using **OpenAI CLIP**, stores them in a **FAISS vector database**, and allows **text or image queries** to retrieve the most similar items.

This project demonstrates a minimal, endâ€‘toâ€‘end **multimodal search system** using:
- CLIP (Contrastive Languageâ€“Image Pretraining)
- FAISS (Facebook AI Similarity Search)
- PyTorch + Transformers

---

## ðŸš€ Features
- Embed **text documents** using CLIP text encoder  
- Embed **images** using CLIP image encoder  
- Store all embeddings in a FAISS index  
- Query using:
  - **Text â†’ retrieve similar texts/images**
  - **Image â†’ retrieve similar texts/images**
- Cosineâ€‘similarity based ranking  
- Fully GPUâ€‘accelerated if CUDA is available  

---
ip install faiss-cpu
pip install transformers pillow numpy
